{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Topic Modeling of Recipes \n",
    "- As a first stab, let's try to use only `Titles`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from recipeScripts import *\n",
    "from miscScripts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that we have the nltk libraries\n",
    "nltk.data.path.append('/Users/jhonsen/Documents/DS/nltk_data/')\n",
    "nltk.download('wordnet', download_dir='/Users/jhonsen/Documents/DS/nltk_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe\n",
    "with open('df_epi_cleaner.pkl','rb') as fin:\n",
    "    df = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn titles into lowercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(lambda word: word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of words in title\n",
    "df.title_numWords.plot(kind='hist', bins=20)\n",
    "plt.title('number of words in title');\n",
    "\n",
    "# Most titles have words between 1 and 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check different types of dishes based on `Titles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout different cuisines of the world \n",
    "# printNum() is imported from recipeScripts.py\n",
    "printNum(['korean','chinese','japanese','italian','french','mexican','indian','thai',\n",
    "          'cajun','vietnamese','american','german','spanish','mediterranian','polish',\n",
    "         'greek','jamaican','african','ethiopian','turkish','indonesian'],\n",
    "                dish_type = 'cuisine').reset_index(drop=True).sort_values(by='number',\n",
    "                                                                         ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out different types of dishes \n",
    "printNum(['salad','barbecue','roast','pizza','soup',\n",
    "              'curry','pasta','antipasti','bbq',\n",
    "              'stew','cake','cookie','wrap','sandwich',\n",
    "          'chicken'],\n",
    "        dish_type='type').reset_index(drop=True).sort_values(by='number', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common words in recipe titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create words out of title\n",
    "df['words'] = df['title'].progress_apply(lambda sent: [word for word in sent.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with processed-Text\n",
    "df['words'] = df['words'].progress_apply(preprocessText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all words in titles into a single bag-of-words\n",
    "# bag_of_texts = reduce(lambda x,y: x+y, df_title.texts.tolist()) #<<--takes long time\n",
    "bag_of_texts = list(chain.from_iterable(df['words'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most frequently-used words\n",
    "freq_words = nltk.FreqDist(bag_of_texts)\n",
    "freq_words.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if we are using TfIdf on tokenized \n",
    "def dummy_func(doc):\n",
    "    return doc\n",
    "\n",
    "# df_title['texts'] is the already tokenized document\n",
    "tokenized_docs = df['words'].tolist()\n",
    "\n",
    "# vectorize documents with TF-IDF\n",
    "tfidfVectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  tokenizer=dummy_func,\n",
    "                                  preprocessor=dummy_func,\n",
    "                                  token_pattern=None,\n",
    "                                  ngram_range=(1,1))\n",
    "# Fit transform\n",
    "tfidfVectorizer.fit(tokenized_docs)\n",
    "dtm = tfidfVectorizer.transform(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dtm in a dataframe\n",
    "df_dtm = pd.DataFrame(dtm.toarray(),\n",
    "             index= df.title,\n",
    "            columns = tfidfVectorizer.get_feature_names())\n",
    "df_dtm.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an NMF with 20 topics\n",
    "\n",
    "nmf = NMF(20)\n",
    "nmf.fit(dtm)\n",
    "nmf_topics = nmf.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe of words in dtm   \n",
    "df_topics = pd.DataFrame(nmf.components_.round(3),\n",
    "                         index=[str(k+1) for k in range(20)],\n",
    "                         columns= tfidfVectorizer.get_feature_names())\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of Observations vs TOPICS\n",
    "\n",
    "df_obs_topics = pd.DataFrame(nmf_topics.round(3),\n",
    "                index= df.title,\n",
    "                columns = [str(k+1) for k in range(20)])\n",
    "df_obs_topics.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking keywords in topics\n",
    "# display_topics() is imported from miscScripts.py \n",
    "display_topics(nmf, tfidfVectorizer.get_feature_names(), no_top_words=8)\n",
    "\n",
    "# The keywords shown below (topics) have reasonable consistency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a `label` column using the NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the datafame\n",
    "## labelTopic() is imported from recipeScripts.py\n",
    "df_labeled = labelTopic(df_obs_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further group the 20 topics into 5 main categories\n",
    "# subcatToMaincat() is imported from recipeScripts.py\n",
    "df['subCat'] = df_labeled['label']\n",
    "df = subcatToMaincat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new Category columns\n",
    "\n",
    "with open('df_epi_cleaner-5.pkl','wb') as fout:\n",
    "    pickle.dump(df_all, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
